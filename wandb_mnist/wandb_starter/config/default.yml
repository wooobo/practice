experiment:
  project_name: "cnn-training"
  experiment_name: "experiment-1"
  group_name: "KTH-CNN"

training:
  epochs: 2
  batch_size: 128
  learning_rate: 0.001
  optimizer: "Adam"
  criterion: "MSELoss"

dataset:
  shuffle: True
  batch_size: 64

device:
  use_cuda: True

# 로깅 및 WandB 설정
wandb:
  use_wandb: true
  save_model: false

# criterion
#CrossEntropyLoss	분류(classification) 문제에서 사용 (Softmax 포함)
#MSELoss	평균 제곱 오차 (회귀 문제에서 사용)
#L1Loss	평균 절대 오차(Mean Absolute Error, MAE)
#SmoothL1Loss	Huber Loss (L1과 L2의 조합)
#HuberLoss	MSE와 L1을 절충한 손실
#BCEWithLogitsLoss	Binary Cross Entropy (Sigmoid 포함)
#BCELoss	Binary Cross Entropy (Sigmoid 없이)
#NLLLoss	음의 로그 우도 손실 (Softmax 필요)
#HingeEmbeddingLoss	SVM에서 사용하는 Hinge 손실
#KLDivLoss	Kullback-Leibler Divergence (확률 분포 비교)


# optim
# ASGD = {type} <class 'torch.optim.asgd.ASGD'>
#Adadelta = {type} <class 'torch.optim.adadelta.Adadelta'>
#Adafactor = {type} <class 'torch.optim.Adafactor'>
#Adagrad = {type} <class 'torch.optim.adagrad.Adagrad'>
#Adam = {type} <class 'torch.optim.adam.Adam'>
#AdamW = {type} <class 'torch.optim.adamw.AdamW'>
#Adamax = {type} <class 'torch.optim.adamax.Adamax'>
#LBFGS = {type} <class 'torch.optim.lbfgs.LBFGS'>
#NAdam = {type} <class 'torch.optim.nadam.NAdam'>

#Optimizer = {type} <class 'torch.optim.optimizer.Optimizer'>
#RAdam = {type} <class 'torch.optim.radam.RAdam'>
#RMSprop = {type} <class 'torch.optim.rmsprop.RMSprop'>
#Rprop = {type} <class 'torch.optim.rprop.Rprop'>
#SGD = {type} <class 'torch.optim.sgd.SGD'>
#SparseAdam = {type} <class 'torch.optim.sparse_adam.SparseAdam'>
#lr_scheduler = {module} <module 'torch.optim.lr_scheduler' from '/Users/slowin/miniconda3/envs/practice_v1/lib/python3.10/site-packages/torch/optim/lr_scheduler.py'>
#swa_utils = {module} <module 'torch.optim.swa_utils' from '/Users/slowin/miniconda3/envs/practice_v1/lib/python3.10/site-packages/torch/optim/swa_utils.py'>